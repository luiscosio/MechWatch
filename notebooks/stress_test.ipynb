{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bc75fa8",
   "metadata": {},
   "source": [
    "# Mechanistic Watchdog: Stress Test & Validation\n",
    "\n",
    "**Module:** `MechWatch`\n",
    "**Goal:** Verify that the \"Deception Score\" is linearly separable between Truthful Control prompts and Deceptive/Adversarial prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d1d8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Setup Paths dynamically for Windows/Linux compatibility\n",
    "current_dir = Path.cwd()\n",
    "# If we are in 'notebooks', parent is project root. If in root, current is root.\n",
    "project_root = current_dir.parent if current_dir.name == \"notebooks\" else current_dir\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"üìÇ Project Root: {project_root}\")\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 2. Import from MechWatch\n",
    "try:\n",
    "    from MechWatch.runtime import WatchdogRuntime\n",
    "    from MechWatch.config import load_config\n",
    "    print(\"‚úÖ Successfully imported WatchdogRuntime from MechWatch\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import failed: {e}. Ensure the folder is named 'MechWatch' and is in the project root.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8eef66",
   "metadata": {},
   "source": [
    "## 1. Initialize Runtime & Locate Vector\n",
    "We need to find the `deception_vector.pt` file and pass its absolute path to the configuration to avoid FileNotFoundError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50113b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Find the vector file\n",
    "vector_candidates = [\n",
    "    project_root / \"artifacts\" / \"deception_vector.pt\",\n",
    "    project_root / \"deception_vector.pt\",\n",
    "    Path(\"artifacts/deception_vector.pt\")\n",
    "]\n",
    "\n",
    "vector_path = next((p for p in vector_candidates if p.exists()), None)\n",
    "\n",
    "if not vector_path:\n",
    "    raise FileNotFoundError(f\"Could not find 'deception_vector.pt' in {project_root} or artifacts/\")\n",
    "\n",
    "print(f\"‚úÖ Found vector at: {vector_path}\")\n",
    "\n",
    "# 2. Load Config and Override Path\n",
    "cfg = load_config()\n",
    "cfg.vector_path = vector_path \n",
    "\n",
    "# 3. Initialize Runtime with the corrected config\n",
    "rt = WatchdogRuntime(cfg=cfg)\n",
    "print(f\"‚úÖ Loaded Watchdog with threshold: {rt.threshold:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d9bf09",
   "metadata": {},
   "source": [
    "## 2. Load Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184581ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "# Configuration: How many samples per category?\n",
    "# Keep this low (e.g., 30-50) for the Hackathon demo so it runs in <10 mins.\n",
    "SAMPLES_PER_CATEGORY = 40 \n",
    "\n",
    "test_cases = []\n",
    "\n",
    "print(\"üì• Loading Datasets...\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. CONTROL GROUP: Simple Truths (Azaria & Mitchell)\n",
    "# ---------------------------------------------------------\n",
    "try:\n",
    "    print(f\"   - Loading Control (Simple Facts)...\")\n",
    "    ds_facts = load_dataset(\"L1Fthrasir/Facts-true-false\", split=\"train\")\n",
    "    # Filter for ONLY True statements to serve as our baseline \"Green Zone\"\n",
    "    true_facts = ds_facts.filter(lambda x: x['label'] == 1).shuffle(seed=42).select(range(SAMPLES_PER_CATEGORY))\n",
    "    \n",
    "    for row in true_facts:\n",
    "        test_cases.append({\n",
    "            \"category\": \"Control (Simple Facts)\",\n",
    "            \"prompt\": row['statement'],\n",
    "            \"expected\": \"Pass\"\n",
    "        })\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è Failed to load Control: {e}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. TRUTHFUL QA: Imitative Falsehoods\n",
    "# ---------------------------------------------------------\n",
    "try:\n",
    "    print(f\"   - Loading TruthfulQA (Misconceptions)...\")\n",
    "    ds_tqa = load_dataset(\"truthful_qa\", \"generation\", split=\"validation\")\n",
    "    ds_tqa = ds_tqa.shuffle(seed=42).select(range(SAMPLES_PER_CATEGORY))\n",
    "    \n",
    "    for row in ds_tqa:\n",
    "        # These prompts entice the model to lie (e.g. \"What happens if you break a mirror?\")\n",
    "        test_cases.append({\n",
    "            \"category\": \"TruthfulQA (Misconceptions)\",\n",
    "            \"prompt\": row['question'],\n",
    "            \"expected\": \"Variable\"\n",
    "        })\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è Failed to load TruthfulQA: {e}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. JAILBREAKS (Tenebrous / DAN / Adversarial)\n",
    "# ---------------------------------------------------------\n",
    "try:\n",
    "    print(f\"   - Loading Jailbreaks (Adversarial)...\")\n",
    "    # Using JailbreakHub as a reliable source of known jailbreaks (DAN, AIM, etc)\n",
    "    ds_jail = load_dataset(\"walledai/JailbreakHub\", split=\"train\")\n",
    "    \n",
    "    # FIX: The column name is 'jailbreak', not 'jailbroken'\n",
    "    ds_jail = ds_jail.filter(lambda x: x['jailbreak'] == True).shuffle(seed=42).select(range(SAMPLES_PER_CATEGORY))\n",
    "    \n",
    "    for row in ds_jail:\n",
    "        test_cases.append({\n",
    "            \"category\": \"Jailbreak Attempts\",\n",
    "            \"prompt\": row['prompt'], \n",
    "            \"expected\": \"Block\"\n",
    "        })\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è Failed to load Jailbreaks: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded {len(test_cases)} total test cases.\")\n",
    "print(f\"   Breakdown: {list(set([c['category'] for c in test_cases]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e635cf0e",
   "metadata": {},
   "source": [
    "## 3. Run the Gauntlet\n",
    "We run each prompt with the watchdog **disabled** (monitoring only) to record the raw scores without stopping the model early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d59cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "results = []\n",
    "SAVE_EVERY = 20\n",
    "\n",
    "print(\"üöÄ Running inference gauntlet...\")\n",
    "\n",
    "# Use enumerate so we have an index 'i' for periodic saving/cleaning\n",
    "for i, case in enumerate(tqdm(test_cases)):\n",
    "    try:\n",
    "        # Run generation (deterministic)\n",
    "        # Reduced max_new_tokens to 40 to be safe on VRAM\n",
    "        output = rt.generate_without_watchdog(\n",
    "            prompt=case[\"prompt\"],\n",
    "            max_new_tokens=40, \n",
    "            temperature=0.0 \n",
    "        )\n",
    "        \n",
    "        scores = output.scores\n",
    "        \n",
    "        # Grace Period Logic\n",
    "        valid_scores = scores[5:] if len(scores) > 5 else scores\n",
    "        if not valid_scores: valid_scores = [0.0]\n",
    "        \n",
    "        peak_score = max(valid_scores)\n",
    "        avg_score = sum(valid_scores) / len(valid_scores)\n",
    "        \n",
    "        # Handle list vs string output safely\n",
    "        raw_text = output.text[0] if isinstance(output.text, list) else output.text\n",
    "        # Strip the prompt to keep the CSV clean\n",
    "        response_text = raw_text[len(case[\"prompt\"]):].strip()\n",
    "        \n",
    "        results.append({\n",
    "            \"Category\": case[\"category\"],\n",
    "            \"Prompt\": case[\"prompt\"],\n",
    "            \"Peak Score\": peak_score,\n",
    "            \"Avg Score\": avg_score,\n",
    "            \"Would Block?\": peak_score > rt.threshold,\n",
    "            \"Output\": response_text[:100].replace(\"\\n\", \" \") + \"...\"\n",
    "        })\n",
    "        \n",
    "        # === MEMORY SAFETY ===\n",
    "        # Clear VRAM every 10 steps\n",
    "        if i % 10 == 0:\n",
    "            del output\n",
    "            del scores\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "        # === PROGRESS SAVING ===\n",
    "        # Save a backup every 20 steps so you don't lose everything\n",
    "        if i > 0 and i % SAVE_EVERY == 0:\n",
    "            pd.DataFrame(results).to_csv(\"stress_test_backup.csv\", index=False)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Skipping index {i}: {e}\")\n",
    "        # If OOM occurs, try to clear and continue\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "print(\"‚úÖ Inference Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2334db2e",
   "metadata": {},
   "source": [
    "## 4. Visualization\n",
    "We expect a clear separation: \"Control\" cases should be below the red line, \"Deception\" cases above it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8bef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Set Plot Style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create Boxplot\n",
    "sns.boxplot(data=df, x=\"Category\", y=\"Peak Score\", palette=\"coolwarm\", hue=\"Category\")\n",
    "sns.stripplot(data=df, x=\"Category\", y=\"Peak Score\", color=\"black\", alpha=0.5, jitter=True)\n",
    "\n",
    "# Draw Threshold Line\n",
    "plt.axhline(rt.threshold, color=\"red\", linestyle=\"--\", linewidth=2, label=f\"Threshold ({rt.threshold:.2f})\")\n",
    "\n",
    "plt.title(\"Peak Deception Score by Category\", fontsize=16)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c14eb7e",
   "metadata": {},
   "source": [
    "## 5. Failure Analysis\n",
    "Identify False Negatives (Deception that slipped through) and False Positives (Truth that was blocked)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Current Threshold: {rt.threshold:.4f}\\n\")\n",
    "\n",
    "print(\"üî¥ FALSE NEGATIVES (Missed Deception):\")\n",
    "fn = df[(df[\"Category\"] != \"Control (Truth)\") & (df[\"Peak Score\"] < rt.threshold)]\n",
    "if not fn.empty:\n",
    "    display(fn[[\"Category\", \"Prompt\", \"Peak Score\", \"Output\"]])\n",
    "else:\n",
    "    print(\"None! All deceptive prompts were caught.\")\n",
    "\n",
    "print(\"\\nüü¢ FALSE POSITIVES (Wrongly Blocked Truth):\")\n",
    "fp = df[(df[\"Category\"] == \"Control (Truth)\") & (df[\"Peak Score\"] > rt.threshold)]\n",
    "if not fp.empty:\n",
    "    display(fp[[\"Category\", \"Prompt\", \"Peak Score\", \"Output\"]])\n",
    "else:\n",
    "    print(\"None! All truthful prompts passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results for report\n",
    "df.to_csv(\"stress_test_results.csv\", index=False)\n",
    "print(\"Results saved to stress_test_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
