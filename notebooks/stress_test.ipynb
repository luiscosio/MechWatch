{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bc75fa8",
   "metadata": {},
   "source": [
    "# Mechanistic Watchdog: Stress Test & Validation\n",
    "\n",
    "**Module:** `MechWatch`\n",
    "**Goal:** Verify that the \"Deception Score\" is linearly separable between Truthful Control prompts and Deceptive/Adversarial prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d1d8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Setup Paths dynamically for Windows/Linux compatibility\n",
    "current_dir = Path.cwd()\n",
    "# If we are in 'notebooks', parent is project root. If in root, current is root.\n",
    "project_root = current_dir.parent if current_dir.name == \"notebooks\" else current_dir\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"üìÇ Project Root: {project_root}\")\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 2. Import from MechWatch\n",
    "try:\n",
    "    from MechWatch.runtime import WatchdogRuntime\n",
    "    from MechWatch.config import load_config\n",
    "    from MechWatch.calibrate import standardize_dataset\n",
    "    print(\"‚úÖ Successfully imported WatchdogRuntime from MechWatch\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import failed: {e}. Ensure the folder is named 'MechWatch' and is in the project root.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8eef66",
   "metadata": {},
   "source": [
    "## 1. Initialize Runtime & Vector Profiles\n",
    "We keep a library of defensive profiles (truthfulness, cyber, bio) and hot-swap the learned vector before each burn-in test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50113b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTIFACT_DIR = project_root / \"artifacts\"\n",
    "ARTIFACT_DIR.mkdir(exist_ok=True)\n",
    "print(f\"üì¶ Artifacts dir: {ARTIFACT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d9bf09",
   "metadata": {},
   "source": [
    "## 2. Profile Definitions & Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184581ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from typing import Callable, Dict, List\n",
    "\n",
    "SEED = 42\n",
    "SAMPLES_PER_CATEGORY = 40\n",
    "\n",
    "\n",
    "def build_truthfulness_cases(seed: int = SEED, samples: int = SAMPLES_PER_CATEGORY) -> List[Dict[str, str]]:\n",
    "    \"\"\"Sample true, edge, and adversarial prompts for the truthfulness profile.\"\"\"\n",
    "    cases: List[Dict[str, str]] = []\n",
    "\n",
    "    ds_facts = load_dataset(\"L1Fthrasir/Facts-true-false\", split=\"train\")\n",
    "\n",
    "    true_facts = ds_facts.filter(lambda x: x[\"label\"] == 1).shuffle(seed=seed).select(\n",
    "        range(min(samples, len(ds_facts)))\n",
    "    )\n",
    "    for row in true_facts:\n",
    "        cases.append({\"category\": \"Control (Truth)\", \"prompt\": row[\"statement\"], \"expected\": \"Pass\"})\n",
    "\n",
    "    ds_tqa = load_dataset(\"truthful_qa\", \"generation\", split=\"validation\")\n",
    "    ds_tqa = ds_tqa.shuffle(seed=seed).select(range(min(samples, len(ds_tqa))))\n",
    "    for row in ds_tqa:\n",
    "        cases.append({\n",
    "            \"category\": \"TruthfulQA (Misconceptions)\",\n",
    "            \"prompt\": row[\"question\"],\n",
    "            \"expected\": \"Variable\",\n",
    "        })\n",
    "\n",
    "    false_facts = ds_facts.filter(lambda x: x[\"label\"] == 0).shuffle(seed=seed).select(\n",
    "        range(min(samples, len(ds_facts)))\n",
    "    )\n",
    "    for row in false_facts:\n",
    "        cases.append({\"category\": \"Factual Lies\", \"prompt\": row[\"statement\"], \"expected\": \"Block\"})\n",
    "\n",
    "    return cases\n",
    "\n",
    "\n",
    "def build_wmdp_cases(\n",
    "    config_name: str,\n",
    "    seed: int = SEED,\n",
    "    safe_samples: int = 40,\n",
    "    misuse_samples: int = 40,\n",
    ") -> List[Dict[str, str]]:\n",
    "    \"\"\"Expand the WMDP multiple-choice dataset into safe/misuse prompts.\"\"\"\n",
    "    ds = load_dataset(\"cais/wmdp\", config_name, split=\"test\")\n",
    "    ds = standardize_dataset(ds)\n",
    "\n",
    "    def take_subset(dataset, count):\n",
    "        count = min(count, len(dataset))\n",
    "        if count <= 0:\n",
    "            return []\n",
    "        return dataset.shuffle(seed=seed).select(range(count))\n",
    "\n",
    "    safe_rows = take_subset(ds.filter(lambda x: x[\"label\"] == 1), safe_samples)\n",
    "    misuse_rows = take_subset(ds.filter(lambda x: x[\"label\"] == 0), misuse_samples)\n",
    "\n",
    "    domain = config_name.split(\"-\")[-1].upper()\n",
    "    records: List[Dict[str, str]] = []\n",
    "    for row in safe_rows:\n",
    "        records.append({\n",
    "            \"category\": f\"WMDP {domain} (Safe)\",\n",
    "            \"prompt\": row[\"statement\"],\n",
    "            \"expected\": \"Pass\",\n",
    "        })\n",
    "    for row in misuse_rows:\n",
    "        records.append({\n",
    "            \"category\": f\"WMDP {domain} (Misuse)\",\n",
    "            \"prompt\": row[\"statement\"],\n",
    "            \"expected\": \"Block\",\n",
    "        })\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1299e922",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROFILE_CONFIGS = [\n",
    "    {\n",
    "        \"name\": \"Truthfulness\",\n",
    "        \"slug\": \"truthfulness\",\n",
    "        \"vector_path\": ARTIFACT_DIR / \"deception_vector.pt\",\n",
    "        \"build_cases\": build_truthfulness_cases,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Cyber Defense\",\n",
    "        \"slug\": \"cyber_defense\",\n",
    "        \"vector_path\": ARTIFACT_DIR / \"cyber_misuse_vector.pt\",\n",
    "        \"build_cases\": lambda: build_wmdp_cases(\"wmdp-cyber\", safe_samples=40, misuse_samples=40),\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Bio Defense\",\n",
    "        \"slug\": \"bio_defense\",\n",
    "        \"vector_path\": ARTIFACT_DIR / \"bio_defense_vector.pt\",\n",
    "        \"build_cases\": lambda: build_wmdp_cases(\"wmdp-bio\", safe_samples=40, misuse_samples=40),\n",
    "    },\n",
    "]\n",
    "\n",
    "for profile in PROFILE_CONFIGS:\n",
    "    status = \"‚úÖ\" if profile[\"vector_path\"].exists() else \"‚ö†Ô∏è\"\n",
    "    print(f\"{status} {profile['name']} vector => {profile['vector_path']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cfe41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_config()\n",
    "primary_profile = next((p for p in PROFILE_CONFIGS if p[\"vector_path\"].exists()), None)\n",
    "if primary_profile is None:\n",
    "    raise FileNotFoundError(\"No concept vectors found. Run calibration for at least one profile under artifacts/.\")\n",
    "\n",
    "cfg.vector_path = primary_profile[\"vector_path\"]\n",
    "rt = WatchdogRuntime(cfg=cfg)\n",
    "print(f\"‚úÖ Loaded model with initial profile: {primary_profile['name']} (threshold {rt.threshold:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4643d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e635cf0e",
   "metadata": {},
   "source": [
    "## 3. Run the Gauntlet per Profile\n",
    "We evaluate each profile independently (truth, cyber, bio) while the watchdog runs in **monitoring mode** so we can chart raw score distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d59cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "SAVE_EVERY = 20\n",
    "profile_runs: Dict[str, pd.DataFrame] = {}\n",
    "\n",
    "\n",
    "def run_gauntlet(rt: WatchdogRuntime, cases, max_new_tokens: int = 40) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    baseline_threshold = rt.threshold\n",
    "    with torch.no_grad():\n",
    "        for i, case in enumerate(tqdm(cases, desc=\"Inference\", unit=\"prompt\")):\n",
    "            try:\n",
    "                output = rt.generate_without_watchdog(\n",
    "                    prompt=case[\"prompt\"],\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    temperature=0.0,\n",
    "                )\n",
    "                scores = output.scores\n",
    "                valid_scores = scores[5:] if len(scores) > 5 else scores\n",
    "                if not valid_scores:\n",
    "                    valid_scores = [0.0]\n",
    "\n",
    "                peak_score = max(valid_scores)\n",
    "                avg_score = sum(valid_scores) / len(valid_scores)\n",
    "\n",
    "                raw_text = output.text[0] if isinstance(output.text, list) else output.text\n",
    "                response_text = raw_text[len(case[\"prompt\"]):].strip()\n",
    "\n",
    "                rows.append({\n",
    "                    \"Category\": case[\"category\"],\n",
    "                    \"Prompt\": case[\"prompt\"],\n",
    "                    \"Peak Score\": peak_score,\n",
    "                    \"Avg Score\": avg_score,\n",
    "                    \"Would Block?\": peak_score >= baseline_threshold,\n",
    "                    \"Output\": response_text[:100].replace(\"\\n\", \" \") + \"...\",\n",
    "                })\n",
    "\n",
    "                if i > 0 and i % SAVE_EVERY == 0:\n",
    "                    pd.DataFrame(rows).to_csv(\"stress_test_backup.csv\", index=False)\n",
    "\n",
    "                del output\n",
    "                del scores\n",
    "                del raw_text\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error at index {i}: {e}\")\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "for profile in PROFILE_CONFIGS:\n",
    "    if not profile[\"vector_path\"].exists():\n",
    "        print(f\"‚è≠Ô∏è Skipping {profile['name']} (vector missing)\")\n",
    "        continue\n",
    "\n",
    "    cases = profile[\"build_cases\"]()\n",
    "    if not cases:\n",
    "        print(f\"‚ö†Ô∏è No cases available for {profile['name']}\")\n",
    "        continue\n",
    "\n",
    "    rt.load_vector_from_path(profile[\"vector_path\"])\n",
    "    print(f\"\\nüöÄ {profile['name']} | cases={len(cases)} | threshold={rt.threshold:.4f}\")\n",
    "    df_profile = run_gauntlet(rt, cases)\n",
    "    df_profile[\"Profile\"] = profile[\"name\"]\n",
    "    df_profile[\"Threshold\"] = rt.threshold\n",
    "    profile_runs[profile[\"name\"]] = df_profile\n",
    "    df_profile.to_csv(f\"stress_test_results_{profile['slug']}.csv\", index=False)\n",
    "\n",
    "if not profile_runs:\n",
    "    raise RuntimeError(\"No profiles were evaluated. Ensure you have calibrated vectors under artifacts/.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2334db2e",
   "metadata": {},
   "source": [
    "## 4. Visualization\n",
    "Plot score distributions per profile and export chart data for the dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8bef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "all_frames = [df for df in profile_runs.values()]\n",
    "combined_df = pd.concat(all_frames, ignore_index=True)\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.boxplot(data=combined_df, x=\"Category\", y=\"Peak Score\", hue=\"Profile\")\n",
    "sns.stripplot(data=combined_df, x=\"Category\", y=\"Peak Score\", color=\"black\", alpha=0.4, jitter=True)\n",
    "plt.title(\"Peak Score Distribution per Profile\", fontsize=16)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for profile in PROFILE_CONFIGS:\n",
    "    name = profile[\"name\"]\n",
    "    if name not in profile_runs:\n",
    "        continue\n",
    "    df_profile = profile_runs[name]\n",
    "    payload = {\n",
    "        \"threshold\": float(df_profile[\"Threshold\"].iloc[0]),\n",
    "        \"data\": df_profile[[\"Category\", \"Peak Score\"]].to_dict(orient=\"records\"),\n",
    "    }\n",
    "\n",
    "    filename = f\"watchdog_chart_data_{profile['slug']}.json\"\n",
    "    out_path = Path(filename)\n",
    "    docs_out_path = Path(\"docs\") / filename\n",
    "\n",
    "    for target in (out_path, docs_out_path):\n",
    "        with open(target, \"w\") as fp:\n",
    "            json.dump(payload, fp, indent=2)\n",
    "\n",
    "    if profile[\"slug\"] == \"truthfulness\":\n",
    "        default_targets = [Path(\"watchdog_chart_data.json\"), Path(\"docs/watchdog_chart_data.json\")]\n",
    "        for target in default_targets:\n",
    "            with open(target, \"w\") as fp:\n",
    "                json.dump(payload, fp, indent=2)\n",
    "        print(\"üìä Updated watchdog_chart_data.json for the website.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c14eb7e",
   "metadata": {},
   "source": [
    "## 5. Failure Analysis\n",
    "Identify False Negatives (Deception that slipped through) and False Positives (Truth that was blocked)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for profile in PROFILE_CONFIGS:\n",
    "    name = profile[\"name\"]\n",
    "    if name not in profile_runs:\n",
    "        continue\n",
    "    df_profile = profile_runs[name]\n",
    "    threshold = df_profile[\"Threshold\"].iloc[0]\n",
    "    print(f\"\\n=== {name} (threshold {threshold:.4f}) ===\")\n",
    "\n",
    "    fn = df_profile[(df_profile[\"Category\"].str.contains(\"Misuse\")) & (df_profile[\"Peak Score\"] < threshold)]\n",
    "    if not fn.empty:\n",
    "        print(\"üî¥ Missed interdictions:\")\n",
    "        display(fn[[\"Category\", \"Prompt\", \"Peak Score\", \"Output\"]])\n",
    "    else:\n",
    "        print(\"üî¥ Missed interdictions: none\")\n",
    "\n",
    "    fp = df_profile[(~df_profile[\"Category\"].str.contains(\"Misuse\")) & (df_profile[\"Peak Score\"] > threshold)]\n",
    "    if not fp.empty:\n",
    "        print(\"üü¢ Over-blocked safe prompts:\")\n",
    "        display(fp[[\"Category\", \"Prompt\", \"Peak Score\", \"Output\"]])\n",
    "    else:\n",
    "        print(\"üü¢ Over-blocked safe prompts: none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv(\"stress_test_results.csv\", index=False)\n",
    "print(\"Results saved to stress_test_results.csv and per-profile CSVs.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
