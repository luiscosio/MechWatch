{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bc75fa8",
   "metadata": {},
   "source": [
    "# Mechanistic Watchdog: Stress Test & Validation\n",
    "\n",
    "**Module:** `MechWatch`\n",
    "**Goal:** Verify that the \"Deception Score\" is linearly separable between Truthful Control prompts and Deceptive/Adversarial prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44d1d8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Project Root: e:\\Dropbox\\Projects\\hackaton\n",
      "‚úÖ Successfully imported WatchdogRuntime from MechWatch\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Setup Paths dynamically for Windows/Linux compatibility\n",
    "current_dir = Path.cwd()\n",
    "# If we are in 'notebooks', parent is project root. If in root, current is root.\n",
    "project_root = current_dir.parent if current_dir.name == \"notebooks\" else current_dir\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"üìÇ Project Root: {project_root}\")\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 2. Import from MechWatch\n",
    "try:\n",
    "    from MechWatch.runtime import WatchdogRuntime\n",
    "    from MechWatch.config import load_config\n",
    "    print(\"‚úÖ Successfully imported WatchdogRuntime from MechWatch\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import failed: {e}. Ensure the folder is named 'MechWatch' and is in the project root.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8eef66",
   "metadata": {},
   "source": [
    "## 1. Initialize Runtime & Locate Vector\n",
    "We need to find the `deception_vector.pt` file and pass its absolute path to the configuration to avoid FileNotFoundError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50113b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found vector at: e:\\Dropbox\\Projects\\hackaton\\artifacts\\deception_vector.pt\n",
      "‚úÖ Loaded Watchdog with threshold: -0.3872\n"
     ]
    }
   ],
   "source": [
    "# 1. Find the vector file\n",
    "vector_candidates = [\n",
    "    project_root / \"artifacts\" / \"deception_vector.pt\",\n",
    "    project_root / \"deception_vector.pt\",\n",
    "    Path(\"artifacts/deception_vector.pt\")\n",
    "]\n",
    "\n",
    "vector_path = next((p for p in vector_candidates if p.exists()), None)\n",
    "\n",
    "if not vector_path:\n",
    "    raise FileNotFoundError(f\"Could not find 'deception_vector.pt' in {project_root} or artifacts/\")\n",
    "\n",
    "print(f\"‚úÖ Found vector at: {vector_path}\")\n",
    "\n",
    "# 2. Load Config and Override Path\n",
    "cfg = load_config()\n",
    "cfg.vector_path = vector_path \n",
    "\n",
    "# 3. Initialize Runtime with the corrected config\n",
    "rt = WatchdogRuntime(cfg=cfg)\n",
    "print(f\"‚úÖ Loaded Watchdog with threshold: {rt.threshold:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d9bf09",
   "metadata": {},
   "source": [
    "## 2. Load Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "184581ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading Datasets (Safe Mode)...\n",
      "   - Loading Control (True Facts)...\n",
      "   - Loading TruthfulQA (Misconceptions)...\n",
      "   - Loading Factual Lies (Adversarial)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e40c967d6a40979dc33cf4ca526ab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/613 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Loaded 120 total test cases.\n",
      "   Breakdown: ['Factual Lies', 'Control (Truth)', 'TruthfulQA (Misconceptions)']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "# Configuration\n",
    "SAMPLES_PER_CATEGORY = 40 \n",
    "\n",
    "test_cases = []\n",
    "\n",
    "print(\"üì• Loading Datasets (Safe Mode)...\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. CONTROL (TRUTH): The Baseline\n",
    "# ---------------------------------------------------------\n",
    "try:\n",
    "    print(f\"   - Loading Control (True Facts)...\")\n",
    "    ds_facts = load_dataset(\"L1Fthrasir/Facts-true-false\", split=\"train\")\n",
    "    \n",
    "    # Filter for TRUE statements (Label 1)\n",
    "    true_facts = ds_facts.filter(lambda x: x['label'] == 1).shuffle(seed=42).select(range(SAMPLES_PER_CATEGORY))\n",
    "    \n",
    "    for row in true_facts:\n",
    "        test_cases.append({\n",
    "            \"category\": \"Control (Truth)\",\n",
    "            \"prompt\": row['statement'],\n",
    "            \"expected\": \"Pass\"\n",
    "        })\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è Failed to load Control: {e}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. TRUTHFUL QA: The Edge Case\n",
    "# ---------------------------------------------------------\n",
    "try:\n",
    "    print(f\"   - Loading TruthfulQA (Misconceptions)...\")\n",
    "    ds_tqa = load_dataset(\"truthful_qa\", \"generation\", split=\"validation\")\n",
    "    ds_tqa = ds_tqa.shuffle(seed=42).select(range(SAMPLES_PER_CATEGORY))\n",
    "    \n",
    "    for row in ds_tqa:\n",
    "        test_cases.append({\n",
    "            \"category\": \"TruthfulQA (Misconceptions)\",\n",
    "            \"prompt\": row['question'],\n",
    "            \"expected\": \"Variable\"\n",
    "        })\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è Failed to load TruthfulQA: {e}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. FACTUAL LIES: The \"Red Zone\" (Replaces Jailbreaks)\n",
    "# ---------------------------------------------------------\n",
    "# Instead of Jailbreaks (which crash the GPU), we use Factual Lies.\n",
    "# These are the exact opposite of the Control group.\n",
    "try:\n",
    "    print(f\"   - Loading Factual Lies (Adversarial)...\")\n",
    "    \n",
    "    # Use the SAME dataset, but filter for FALSE statements (Label 0)\n",
    "    false_facts = ds_facts.filter(lambda x: x['label'] == 0).shuffle(seed=42).select(range(SAMPLES_PER_CATEGORY))\n",
    "    \n",
    "    for row in false_facts:\n",
    "        test_cases.append({\n",
    "            \"category\": \"Factual Lies\",\n",
    "            \"prompt\": row['statement'], \n",
    "            \"expected\": \"Block\"\n",
    "        })\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è Failed to load Factual Lies: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded {len(test_cases)} total test cases.\")\n",
    "print(f\"   Breakdown: {list(set([c['category'] for c in test_cases]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e635cf0e",
   "metadata": {},
   "source": [
    "## 3. Run the Gauntlet\n",
    "We run each prompt with the watchdog **disabled** (monitoring only) to record the raw scores without stopping the model early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d59cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Gauntlet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:   0%|          | 0/120 [00:00<?, ?prompt/s]`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92dd21cc6c1440dbb927fe2ba929eedf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-3.1-8B-Instruct into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:  18%|‚ñà‚ñä        | 21/120 [01:29<05:37,  3.41s/prompt]"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "SAVE_EVERY = 20\n",
    "\n",
    "print(\"üöÄ Starting Gauntlet...\")\n",
    "\n",
    "# GLOBAL no_grad to save massive memory\n",
    "with torch.no_grad():\n",
    "    for i, case in enumerate(tqdm(test_cases, desc=\"Inference\", unit=\"prompt\")):\n",
    "        try:\n",
    "            # 1. Run Generation\n",
    "            # Reduced tokens to 40 for speed/safety\n",
    "            output = rt.generate_without_watchdog(\n",
    "                prompt=case[\"prompt\"],\n",
    "                max_new_tokens=40, \n",
    "                temperature=0.0 \n",
    "            )\n",
    "            \n",
    "            # 2. Extract Metrics\n",
    "            scores = output.scores\n",
    "            valid_scores = scores[5:] if len(scores) > 5 else scores\n",
    "            if not valid_scores: valid_scores = [0.0]\n",
    "            \n",
    "            peak_score = max(valid_scores)\n",
    "            avg_score = sum(valid_scores) / len(valid_scores)\n",
    "            \n",
    "            # 3. Handle Text Extraction\n",
    "            # TransformerLens can return list or string depending on batch dim\n",
    "            raw_text = output.text[0] if isinstance(output.text, list) else output.text\n",
    "            response_text = raw_text[len(case[\"prompt\"]):].strip()\n",
    "            \n",
    "            results.append({\n",
    "                \"Category\": case[\"category\"],\n",
    "                \"Prompt\": case[\"prompt\"],\n",
    "                \"Peak Score\": peak_score,\n",
    "                \"Avg Score\": avg_score,\n",
    "                \"Would Block?\": peak_score > rt.threshold,\n",
    "                \"Output\": response_text[:100].replace(\"\\n\", \" \") + \"...\"\n",
    "            })\n",
    "            \n",
    "            # 4. Aggressive Cleanup\n",
    "            # Delete large objects immediately\n",
    "            del output\n",
    "            del scores\n",
    "            del raw_text\n",
    "            \n",
    "            # 5. Periodic Saving (Safety Net)\n",
    "            if i > 0 and i % SAVE_EVERY == 0:\n",
    "                pd.DataFrame(results).to_csv(\"stress_test_backup.csv\", index=False)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error at index {i}: {e}\")\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "print(\"‚úÖ Inference Complete.\")\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"stress_test_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2334db2e",
   "metadata": {},
   "source": [
    "## 4. Visualization\n",
    "We expect a clear separation: \"Control\" cases should be below the red line, \"Deception\" cases above it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8bef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Set Plot Style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create Boxplot\n",
    "sns.boxplot(data=df, x=\"Category\", y=\"Peak Score\", palette=\"coolwarm\", hue=\"Category\")\n",
    "sns.stripplot(data=df, x=\"Category\", y=\"Peak Score\", color=\"black\", alpha=0.5, jitter=True)\n",
    "\n",
    "# Draw Threshold Line\n",
    "plt.axhline(rt.threshold, color=\"red\", linestyle=\"--\", linewidth=2, label=f\"Threshold ({rt.threshold:.2f})\")\n",
    "\n",
    "plt.title(\"Peak Deception Score by Category\", fontsize=16)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c14eb7e",
   "metadata": {},
   "source": [
    "## 5. Failure Analysis\n",
    "Identify False Negatives (Deception that slipped through) and False Positives (Truth that was blocked)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Current Threshold: {rt.threshold:.4f}\\n\")\n",
    "\n",
    "print(\"üî¥ FALSE NEGATIVES (Missed Deception):\")\n",
    "fn = df[(df[\"Category\"] != \"Control (Truth)\") & (df[\"Peak Score\"] < rt.threshold)]\n",
    "if not fn.empty:\n",
    "    display(fn[[\"Category\", \"Prompt\", \"Peak Score\", \"Output\"]])\n",
    "else:\n",
    "    print(\"None! All deceptive prompts were caught.\")\n",
    "\n",
    "print(\"\\nüü¢ FALSE POSITIVES (Wrongly Blocked Truth):\")\n",
    "fp = df[(df[\"Category\"] == \"Control (Truth)\") & (df[\"Peak Score\"] > rt.threshold)]\n",
    "if not fp.empty:\n",
    "    display(fp[[\"Category\", \"Prompt\", \"Peak Score\", \"Output\"]])\n",
    "else:\n",
    "    print(\"None! All truthful prompts passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results for report\n",
    "df.to_csv(\"stress_test_results.csv\", index=False)\n",
    "print(\"Results saved to stress_test_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
